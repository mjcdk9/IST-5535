---
title: "Lab_2"
author: "Mark Chafin"
date: "2/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# 1. Simulate Data

1. Perform the following commands in R:
The last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model. What are the regression coefficients?
```{r}
set.seed(1)
x1<- runif(100)
x2 <- 0.5* x1 + rnorm(100) /10
y <- 2 + 2* x1 + 0.3* x2 + rnorm(100)
```
The formula of the regression:

$Y=\beta_0 + \beta_1*x_1 +\beta_2*x_2 + \epsilon$

$\beta_0=2$
$\beta_2=2$
$\beta_1=0.3$

# 2. Relationship between X1 and X2

What is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables.

```{r}
cor(x1, x2)
```
```{r}
cor.test(x1, x2)
```
```{r}
library(ggplot2)
qplot(x= x1, y=x2)

```

# 3. OLS: Regression Y on X1 and X2

3. Using this data, fit a least squares regression to predict y using x1 and x2. Describe the results obtained. What are β0, β1, and β2? How do these relate to the true β0, β1, and β2? Can you reject the null hypothesis H0 : β1 = 0? How about the null hypothesis H0 : β2 = 0?



```{r}
mod1 <- lm(y ~ x1 + x2)

summary(mod1)
```
# 4. Regress Y on X1

4. Now fit a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis H0 : β1 = 0?

```{r}
mod2 <-  lm(y ~ x1)

summary(mod2)
```
# 5. Regress Y on x2

5. Now fit a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis H0 : β1 = 0?

```{r}
mod3 <-  lm(y ~ x2)

summary(mod3)
```

# 6. 
6. Do the results obtained in (3)–(5) contradict each other? Explain your answer.

```{r}
library(stargazer)
stargazer(mod1, mod2, mod3,)
```

# 7. Redo analysis with New Data

7. Now suppose we obtain one additional observation, which was unfortunately mismeasured.
x1<- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)
```{r}
x1<- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)
```


Re-fit the linear models from (3) to (5) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.


## 7. Regress Y on x1 and x2

```{r}
mod2_1 <- lm(y ~ x1 + x2)
summary(mod2_1)
```

```{r}
par(mfrow=c(2,2))
plot(mod2_1)
```

leverage > (p+1)/n = (2+1)/101 = `{r}(2+1)/101`
```{r}
plot(mod2_1$fitted.values, rstudent(mod2_1))
```

studentized residuals are in the range [-3, 3], then there is no outliers.

## 7.2. Regress Y on x1 and x1

```{r}
mod2_2 <- lm(y ~ x1)
summary(mod2_2)
```


```{r}
par(mfrow=c(2,2))
plot(mod2_2)
```

```{r}
plot(mod2_2$fitted.values, rstudent(mod2_2))
```

```{r}
mod2_3 <- lm(y ~ x2)
summary(mod2_3)
```
## 7.2 Regress Y on x2

```{r}
mod2_2 <- lm(y ~ x1)
summary(mod2_2)
```
```{r}
mod2_3 <- lm(y ~ x2)
summary(mod2_3)
```
```{r}
plot(mod2_3$fitted.values, y= rstudent(mod2_3))
```

