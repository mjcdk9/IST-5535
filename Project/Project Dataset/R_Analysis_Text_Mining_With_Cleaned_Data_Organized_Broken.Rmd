ab?---
title: "Skyrim Steam Reviews"
author: 'Group 2:  Joseph Bextermueller, Mark Chafin, David Ruitt, and Lauren Shaffer
  IST 5535'
date: "3/16/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
  rm(list = ls())
```

## 1. Read dataset
```{r}
  #install.packages("jsonlite") Will need this
  #install.packages("tm") will need this for text mining
  #install.packages("qdap") used for word_count function
  library(jsonlite)
  The_Elder_Scrolls_JSON <- "The_Elder_Scrolls_V.jsonlines"
  
  df.original <- stream_in(file(The_Elder_Scrolls_JSON))
```

## 2. Analyze__________________________________________________________________________________
##### Number of rows & columns
```{r}
  nrow(df.original)
  ncol(df.original)
```

##### Head
```{r}
  head(df.original)
```

The rows represent the individual steam account associated with each review, and the columns represent the various attributes to each account such as: number of reviews, found helpful/unhelpful, review contents, etc.

##### Barchart comparing helpful reviews and play hours
```{r}
  library(ggplot2)

#100 hours
  ggplot(df.original, aes(x=total_game_hours, fill=num_found_helpful)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(0,100) + ylim(0,10) +
    labs(
      title = "Distribution of game hours and helpful reviews (100 hours)",
      x = "Total game hours",
      y = "Reviews found helpful"
    )

#200 hours
    ggplot(df.original, aes(x=total_game_hours, fill=num_found_helpful)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(100,200) + ylim(0,10) +
    labs(
      title = "Distribution of game hours and helpful reviews (200 hours)",
      x = "Total game hours",
      y = "Reviews found helpful"
    )

#300 hours
      ggplot(df.original, aes(x=total_game_hours, fill=num_found_helpful)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(200,300) + ylim(0,10) +
    labs(
      title = "Distribution of game hours and helpful reviews (100 hours)",
      x = "Total game hours",
      y = "Reviews found helpful"
    )

#400 hours
      ggplot(df.original, aes(x=total_game_hours, fill=num_found_helpful)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(300,400) + ylim(0,10) +
    labs(
      title = "Distribution of game hours and helpful reviews (100 hours)",
      x = "Total game hours",
      y = "Reviews found helpful"
    )
      
#500 hours
      ggplot(df.original, aes(x=total_game_hours, fill=num_found_helpful)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(400,500) + ylim(0,10) +
    labs(
      title = "Distribution of game hours and helpful reviews (100 hours)",
      x = "Total game hours",
      y = "Reviews found helpful"
    )
```

When graphing this we see that playtime and finding reviews helpful varies greatly. The graph was split for accurate reading.

We see that the vast majority of interactions with reviews and play time is within the range of 100 to 200 hours. This makes sense, as players will rarely exceed those playtimes for given games. We see that those with fewer hours interact with reviews far more than those who have more hours. This makes sense practically, as those who put many hours in a video game are less likely to read reviews for a given game, given that they enjoy it enough to spend many hours playing it.

##### Barchart comparing funny reviews and play hours
```{r}
#100 hours
  ggplot(df.original, aes(x=total_game_hours, fill=num_found_funny)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(0,100) + ylim(0,10) +
    labs(
      title = "Distribution of game hours and helpful reviews (100 hours)",
      x = "Total game hours",
      y = "Reviews found funny"
    )

#200 hours
  ggplot(df.original, aes(x=total_game_hours, fill=num_found_funny)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(100,200) + ylim(0,10) +
    labs(
      title = "Distribution of game hours and helpful reviews (100 hours)",
      x = "Total game hours",
      y = "Reviews found funny"
    )
  
#300 hours
  ggplot(df.original, aes(x=total_game_hours, fill=num_found_funny)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(200,300) + ylim(0,10) +
    labs(
      title = "Distribution of game hours and helpful reviews (100 hours)",
      x = "Total game hours",
      y = "Reviews found funny"
    )
  
#400 hours
  ggplot(df.original, aes(x=total_game_hours, fill=num_found_funny)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(300,400) + ylim(0,10) +
    labs(
      title = "Distribution of game hours and helpful reviews (100 hours)",
      x = "Total game hours",
      y = "Reviews found funny"
    )
  
#500 hours
  ggplot(df.original, aes(x=total_game_hours, fill=num_found_funny)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(400,500) + ylim(0,10) +
    labs(
      title = "Distribution of game hours and helpful reviews (100 hours)",
      x = "Total game hours",
      y = "Reviews found funny"
    )
```

The previous findings bring another question, especially with the context of Skyrim. Do those who spend more hours in a game post funny reviews? Many most likely have seen reviews from players with hundreds of hours in the game posting humorous reviews.

However, we see a trend similar to our previous findings.

##### Barchart comparing unhelpful reviews and play hours
```{r}
#100 hours
  ggplot(df.original, aes(x=total_game_hours, fill=num_found_unhelpful)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(0,100) + ylim(0,10) +
    labs(
      title = "Distribution of game hours and helpful reviews (100 hours)",
      x = "Total game hours",
      y = "Reviews found unhelpful"
    )

#200 hours
  ggplot(df.original, aes(x=total_game_hours, fill=num_found_unhelpful)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(100,200) + ylim(0,10) +
    labs(
      title = "Distribution of game hours and helpful reviews (100 hours)",
      x = "Total game hours",
      y = "Reviews found unhelpful"
    )
  
#300 hours
  ggplot(df.original, aes(x=total_game_hours, fill=num_found_unhelpful)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(200,300) + ylim(0,10) +
    labs(
      title = "Distribution of game hours and helpful reviews (100 hours)",
      x = "Total game hours",
      y = "Reviews found unhelpful"
    )
  
#400 hours
  ggplot(df.original, aes(x=total_game_hours, fill=num_found_unhelpful)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(300,400) + ylim(0,10) +
    labs(
      title = "Distribution of game hours and helpful reviews (100 hours)",
      x = "Total game hours",
      y = "Reviews found unhelpful"
    )

#500 hours
  ggplot(df.original, aes(x=total_game_hours, fill=num_found_unhelpful)) + geom_bar(position=position_dodge()) + coord_flip() + xlim(400,500) + ylim(0,10) +
    labs(
      title = "Distribution of game hours and helpful reviews (100 hours)",
      x = "Total game hours",
      y = "Reviews found unhelpful"
    )
```

Again, our trend is similar. The vast majority of interaction occurs with those with game hours between 0 to 200. 

##### Real or fake?

* Now the question is how do we distinguish between a real review and a humorous one? Once we do this, how do they relate with play hours?
```{r}
  #print(df.original$review)
    # Commented out for knitting purposes
```

When examining the reviews we see the vast majority of humor reviews are those that tend to be long. So we can frame the question as: Long reviews vs Short reviews. Then we can compare each with helpfulness/unhelpfullness/funny.
```{r}
  df.original$is_short_review <- ifelse(nchar(df.original$review)<120,1,0)
```

* Now, lets see how length plays a role in categorizing reviews. Due to previous findings we will work within 0-200 hours.
```{r}
  #Short reviews found helpful or unhelpful
  barplot(with(df.original, table(num_found_helpful, is_short_review)), main = "Short Reviews found Helpful or Unhelpful", col=1)

  #Short reviews found funny
  barplot(with(df.original, table(num_found_funny, is_short_review)), main = "Short Reviews found Funny or Unfunny", col=1)
```


In our findings here we see that both short and long reviews are fairly balanced in both categories of helpful/unhelpful and funny/unfunny. However, we see that longer reviews slightly lead in being helpful and funny over shorter ones.

* Out of reviews found helpful/unhelpful, which category tends to also be voted more for being funny?
```{r}
  #Set review is funny boolean
  df.original$reviewIsFunny <- ifelse(df.original$num_found_funny>0,1,0)
  table(df.original$reviewIsFunny)

  ggplot(df.original, aes(x=reviewIsFunny, fill=num_found_helpful)) +
    geom_bar(position=position_dodge()) +
    coord_flip() +
    labs(
    title = "Relationship with Helpful/Unhelpful Reviews being found funny",
    x = "Review is Funny",
    y = "Num Helpful",
    fill = "Region"
  )
```
We see that reviews that are found funny tend to also be those that are generally voted as unhelpful by other users. 


## 3. Text Mining & Analysis _______________________________________________________________________________________________________________________________________________________________________________

#### Next, we can text mine these reviews to see which words tend to be repeated under each category (helpful,unhelpful, funny).
```{r}
# Text Mining set was pulled from "https://www.hackerearth.com/practice/machine-learning/advanced-techniques/text-mining-feature-engineering-r/tutorial/"
library(data.table)
library(tm)

tdata <- df.original

tdata$review <- lapply(tdata$review, tolower)

text_corpus <- Corpus(VectorSource(tdata$review))

# Clean Corpus Data
dropword <- "br"
text_corpus <- tm_map(text_corpus, content_transformer(removeWords),dropword)

text_corpus <- tm_map(text_corpus, content_transformer(tolower))

text_corpus <- tm_map(text_corpus, content_transformer(removePunctuation))

text_corpus <- tm_map(text_corpus, content_transformer(removeNumbers))

text_corpus <- tm_map(text_corpus, content_transformer(removeWords), c(stopwords('english')))

text_corpus <- tm_map(text_corpus, content_transformer(stripWhitespace))

# Stem the document so all plural words are now counted as one word
text_corpus <- tm_map(text_corpus, stemDocument, language = "english")

#print(as.character(text_corpus[[1]]))
#text_corpus[[1]]$content


docterm_corpus <- DocumentTermMatrix(text_corpus)
dim(docterm_corpus)

new_docterm_corpus <- removeSparseTerms(docterm_corpus,sparse = 0.95)
dim(new_docterm_corpus)

#find frequent terms
colS <- colSums(as.matrix(new_docterm_corpus))
length(colS)
doc_features <- data.table(name = attributes(colS)$names, count = colS)

#most frequent and least frequent words
doc_features[order(-count)][1:10] #top 10 most frequent words
doc_features[order(count)][1:10] #least 10 frequent words

processed_data <- as.data.table(as.matrix(new_docterm_corpus))
text_dataset <- cbind(df.original, processed_data)

#head(text_dataset)
  #Commented out for knitting purposes
```


##### Find out what predictors best help our predictions

* First thing to do will be to clean the data
```{r}
library("qdap")
# First going to clean the data to be better used
#These are the two datasets. Text_mining_data includes the text mining information, while original_data does not. There for easier use in analysis comparison.

text_mining_data <- text_dataset
df.clean <- df.original

# Get word count, used for analysis later
text_mining_data$review_word_count <- word_count(text_mining_data$review)

df.clean$review_word_count <- word_count(df.clean$review)

# Change achievement_progress to just hold the achievement percentage as the other data is not helpful.
text_mining_data$achievement_progress <- text_mining_data$achievement_progress$num_achievements_percentage

df.clean$achievement_progress <- df.clean$achievement_progress$num_achievements_percentage

# Remove Unique Identifiers and variables that don't help in predictions
text_mining_data$profile_url <- NULL
text_mining_data$steam_id_number <- NULL
text_mining_data$orig_url <- NULL
text_mining_data$review_url <- NULL
text_mining_data$username <- NULL
text_mining_data$review <- NULL
text_mining_data$date_posted <- NULL
text_mining_data$date_updated <- NULL
text_mining_data$is_short_review <- factor(text_mining_data$is_short_review)
text_mining_data$reviewIsFunny <- factor(text_mining_data$reviewIsFunny)
text_mining_data$rating <- factor(text_mining_data$rating)

# Same for original data set
df.clean$profile_url <- NULL
df.clean$steam_id_number <- NULL
df.clean$orig_url <- NULL
df.clean$review_url <- NULL
df.clean$username <- NULL
df.clean$review <- NULL
df.clean$date_posted <- NULL
df.clean$date_updated <- NULL
df.clean$is_short_review <- factor(df.clean$reviewIsFunny)
df.clean$reviewIsFunny <- factor(df.clean$reviewIsFunny)
df.clean$rating <- factor(df.clean$rating)

# Replace NA values
df.clean$num_badges[is.na(df.clean$num_badges)]<-mean(df.clean$num_badges,na.rm=TRUE)
df.clean$achievement_progress[is.na(df.clean$achievement_progress)]<-mean(df.clean$achievement_progress,na.rm=TRUE)
df.clean$friend_player_level[is.na(df.clean$friend_player_level)]<-mean(df.clean$friend_player_level,na.rm=TRUE)
df.clean$num_friends[is.na(df.clean$num_friends)]<-mean(df.clean$num_friends,na.rm=TRUE)
df.clean$num_groups[is.na(df.clean$num_groups)]<-mean(df.clean$num_groups,na.rm=TRUE)


```

* Split the data set into training and test data
```{r}
  #50/50 split
  splitdata <- sample(1:nrow(df.clean), nrow(df.clean)*0.50)

  train <- df.clean[splitdata,]
  test <- df.clean[-splitdata,]
  
  dim(train)
  
  #50/50 split text mining
  splitminingdata <- sample(1:nrow(text_mining_data), nrow(text_mining_data)*0.50)
  
  train_text <- text_mining_data[splitminingdata,]
  test_text <- text_mining_data[-splitminingdata,]
  
  dim(train_text)
```

##### Analysis on playtime and word count reviews
```{r}
  #Set playtime to two categories, <200 hours and >200 hours
  df.clean$playtime200hours <- ifelse(df.clean$total_game_hours<200,1,0)
  
  #Plot
  ggplot(df.clean, aes(x=playtime200hours, fill=review_word_count)) +
    geom_bar(position=position_dodge()) +
    coord_flip() +
    labs(
    title = "Relationship with Playtime and Review Wordcount",
    x = "Playtime",
    y = "Word Count",
    fill = "Region"
  )
```

From this we see what can be expected. Those who put more than 200 hours into a video game write longer reviews the vast majority of the time. We can potentially link this to our previous finding in that longer reviews were also found to be more funny, as those more familiar with the game may joke about its quirks.

##### Simple Linear Regression on clean data
```{r}
library('caret')

# ToDo make sure this is predicting properly (i got docked on the homework)

#str(df.clean)
logit.fit.clean.train <- glm(found_helpful_percentage ~ ., family = binomial(link='logit'), data = train)
summary(logit.fit.clean.train)

logit.fit.clean.test <- glm(found_helpful_percentage ~ ., family = binomial(link='logit'), data = test)
logit.test_probs <- predict(logit.fit.clean.test, newdata = test, type="response")
#factor(ifelse(logit.test_probs > 0.5, "1", "0"))
levels(factor(ifelse(test$found_helpful_percentage > mean(test$found_helpful_percentage,na.rm=TRUE), "1", "0")))
confusionMatrix(factor(ifelse(logit.test_probs > mean(test$found_helpful_percentage,na.rm=TRUE), "1", "0")), factor(ifelse(test$found_helpful_percentage > mean(test$found_helpful_percentage,na.rm=TRUE), "1", "0")), positive="1")
```


##### Simple Linear Regression on text mining data
```{r}
#str(df.clean)
logit.fit.mining.train <- glm(found_helpful_percentage ~ ., family = binomial(link='logit'), data = train_text)
summary(logit.fit.mining.train)

logit.fit.mining.test <- glm(found_helpful_percentage ~ ., family = binomial(link='logit'), data = test_text)
logit.test_probs <- predict(logit.fit.mining.test, newdata = test_text, type="response")
#factor(ifelse(logit.test_probs > 0.5, "1", "0"))
levels(factor(ifelse(test$found_helpful_percentage > mean(test$found_helpful_percentage,na.rm=TRUE), "1", "0")))
confusionMatrix(factor(ifelse(logit.test_probs > mean(test$found_helpful_percentage,na.rm=TRUE), "1", "0")), factor(ifelse(test$found_helpful_percentage > mean(test$found_helpful_percentage,na.rm=TRUE), "1", "0")), positive="1")
```


##### LDA On Clean Data
```{r}
# Model needs to be evaluated on Test Data Set
library("MASS")

lda.fit.clean <- lda(found_helpful_percentage ~ num_found_unhelpful + rating + num_friends + is_short_review, data = test)

lda.pred <- predict(lda.fit.clean)

lda.pred <- as.data.frame(do.call(cbind.data.frame, lda.pred))

confusionMatrix(factor(ifelse(as.numeric(lda.pred$class) > mean(as.numeric(lda.pred$class),na.rm=TRUE), "1","0")), factor(ifelse(test$found_helpful_percentage > mean(test$found_helpful_percentage,na.rm=TRUE), "1","0")), positive = "1")
```


##### LDA On Text Mining Data
```{r}
lda.fit.clean.text <- lda(found_helpful_percentage ~ rating + reviewIsFunny, data = test_text)

lda.pred.text <- predict(lda.fit.clean.text)

# Trying to convert back to levels
lda.pred.text$class <- as.numeric(as.character(lda.pred.text$class))
summary(lda.pred.text$class)

#lda.pred.text <- as.data.frame(do.call(cbind.data.frame, lda.pred.text))
#class(lda.pred.text)

# Going from a factor to as.numeric return 207 only. This is wrong. Fix it


levels(factor(ifelse(as.numeric(lda.pred.text$class) > mean(as.numeric(lda.pred.text$class),na.rm=TRUE), "1","0")))
levels(factor(ifelse(test_text$found_helpful_percentage > mean(test_text$found_helpful_percentage,na.rm=TRUE), "1","0")))

confusionMatrix(factor(ifelse(as.numeric(lda.pred.text$class) > mean(as.numeric(lda.pred.text$class),na.rm=TRUE), "1","0")), factor(ifelse(test_text$found_helpful_percentage > mean(test_text$found_helpful_percentage,na.rm=TRUE), "1","0")), positive = "1")
```

##### QDA On Clean Data
```{r}
# Model needs to be evaluated on Test Data Set
test <- droplevels(test$rating)

qda.fit.clean <- qda(found_helpful_percentage ~ num_found_unhelpful + rating + num_friends + is_short_review, data = test)

qda.pred <- predict(qda.fit.clean)

qda.pred <- as.data.frame(do.call(cbind.data.frame, qda.pred))

confusionMatrix(factor(ifelse(as.numeric(qda.pred$class) > mean(as.numeric(qda.pred$class),na.rm=TRUE), "1","0")), factor(ifelse(test$found_helpful_percentage > mean(test$found_helpful_percentage,na.rm=TRUE), "1","0")), positive = "1")
```


##### QDA On Text Mining Data
```{r}
qda.fit.clean <- qda(found_helpful_percentage ~ rating + reviewIsFunny, data = test_text)

qda.pred <- predict(qda.fit.clean)

qda.pred <- as.data.frame(do.call(cbind.data.frame, qda.pred))

confusionMatrix(factor(ifelse(as.numeric(qda.pred$class) > mean(as.numeric(qda.pred$class),na.rm=TRUE), "1","0")), factor(ifelse(test_text$found_helpful_percentage > mean(test_text$found_helpful_percentage,na.rm=TRUE), "1","0")), positive = "1")
```


##### Analysis with word count and text mining
```{r}

```

##### Then do analysis
```{r}

```

##### Then do analysis
```{r}

```



